# What Do You Mean: A Bayesian Analysis of Social Misunderstanding

#### by Christy Jestin and Charles Ma

## A final project for 6.412 Computational Cognitive Science

This repository contains all code written for our research project. Please find our final paper [here](https://drive.google.com/file/d/1NayaVf9M8ucLFXL4XjflfkiM8oUCV8h-/view?usp=share_link). Here's a breakdown of all the files that are included:

### Data

**data.csv** is a csv containing all unprocessed data returned by our Qualtrics survey.

**prior_data.csv** and **posterior_data.csv** contain postprocessed data split by the prior and posterior versions of the survey. Both csvs are organized with each row containing the responses of one survey participant with ratings converted to probabilities. This means that within **prior_data.csv**, all P(action|_intention_) values for each of _Friendship, Relationship, Hookup_ sum to 1, while within **posterior_data.csv**, all P(intention| _action_) values for each potential action (potential text messages you receive) also sum to 1. All data processing and normalization were done in **process.ipynb**. All rows are anonymized, with demographics of each respondent stored via the ordering from Qualtrics; this ordering is expressed in `demographic_dict` dictionary in the visualization and calculation notebooks. The columns have been renamed to have more meaningful, human-readable values.

### Code

As mentioned, **process.ipynb** contains all code used to process, normalize, and rekey the raw data before analysis could be performed. **visualization.ipynb** contains all code and graphs for visualizing the results of our data, including full plots of the prior and the posterior across each intention/action, and individual prior/posterior plots broken down by demographic. **calculation.ipynb** contains code written to develop our model to compute priors. We did this by computing the average likelihood and posterior distributions and performing a grid search to check possible values for the prior distribution. We considered models generated by combining the average likelihood distribution and with the potential prior, and we compared the generated posterior to the average observed posterior with KL divergence as our distance function. The prior values with the lowest KL divergence became our optimal prior.
